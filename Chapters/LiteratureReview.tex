

\chapter{Literature Review \label{litReview}}
\subsection{Introduction}
An odd power surge phenomenon was recognized during the 2008 Olympics in the UK. At certain times in  the evening there would be massive power surges at what seemed like random times in the evening. Though they seemed random to the utility companies they realized something  peculier was happening at these random times. Making a cup of tea is deeply ingrained in the english culture, after looking at the data they realized that the power surge was happening during times when a TV commercial would come on. Whenever a commercial came on TV multiple families would switch on their kettles to make a cup of tea. Though a single kettle may seem harmless, hundreds of thousands of kettles switched on at the same time would cause a large effect on the load of the power grid. This phenomenon was called the \textbf{\textit{'Great British Kettle Surge'}} \cite{kettle_surge}.

Whenever the kettles were turned on the load on the grid would increase. Electric load refers to the power demanded by consumers at any given time, typically measures in MegaWatts(MW). It represents the combined consumption of household, businesses and industries connected in a grid and it should meet the requirements of all end users  \cite{dong2021short}. Provision of enough electricity to meet the load demand of the grid is crucial for safe, reliable and economic operation of power systems \cite{dong2021short}.

The effect of this great kettle surge can be very harmful to the grid if necessary steps are not taken to increase the grids capacity in times when we expect the load to increase. This is where the concept of load forecasting comes into play.

Electric load forecasting is the process of predicting how much electricity will be needed at a given time and how that demand will affect the utility grid \cite{IBM_loadforecasting}. In the South African context, this issue is particularly relevant, as the country has faced persistent challenges in meeting the energy needs of its population. These difficulties can be attributed, in part, to the governmentâ€™s slow response to expert recommendations in load forecasting and energy planning, despite growing demand. A growing population results in a growth in the demand of electricity. If the country does not build facilities to supply enough electricity for the load requirements then  load-shedding becomes the only solution to protect the grid. In a perfect country the advice from load forecasters in the early 2000's would have been taken into consideration and built more facilities to meet this demand.
The above example shows the hand in hand relationship between load forecasts and their economic impacts. Large forecasting errors may lead to either risky ory conservative
scheduling, which can in turn result in undesirable economic penalties\cite{festas2001computational}. Reducing prediction error on a 10GW power generation facility by 1\% can save the station up to  \$1.6 million a year \cite{wang2023short}. This means that there is a push towards finding the best load forecasting techniques that can be accurate and have minimal errors all year round.

Load forecasting is separated into 3 main categories which are short , medium and long term load forecasting. the major differentiator between the three is the duration in which the forecasting is predicted for.

\textbf{Long Term Load Forecasting(LTLF)} considers periods that are more than a year. LTLF mainly considers factors such as demographic changes, economic growth and energy policy impacts \cite{IBM_loadforecasting}. This forecasting helps utilities think of what can be done to improve their systems to meet the increasing demand of the grid in the future.

\textbf{Medium Term Load Forecasting (MTLF)} forecasts look at periods between a few months and a year. MTLF is important for demand side management  , storage maintenance and scheduling of power \cite{han2018enhanced}.

\textbf{Short term load forecasting(STLF)} which looks at shorter time periods from hourly, daily all the way up to a week of load prediction. STLF is essential for the daily operation performance of a power grid, estimating how many power generators can be used in a particular day \cite{shohan2022forecasting}. Finding accurate prediction models can not only save money but it will sustain the reliability and stability of a grid while increasing its operational efficiency \cite{wu2020short}. STLF can also aid in resource management on the generation side as well as demand side management of electricity \cite{rafi2021short}, this will help the demand side to plan usage and the supply side use their resources for generation efficiently.

 The demand for more accurate prediction models has been continuously growing as the utilities focus on environmentally friendly and efficient power generation. An efficient and accurate prediction model eliminated the problems such as \textit{british kettle surge} as utilities can plan ahead by ensuring more generators are operational when the demand for power rises. STLF can also ensure that the grid has a reliable continuous power flow during power shortages or outages\cite{tarmanini2023short}.

In this study we look at and evaluate different models that have been used for STLF. Recent studies have shown a promise in using machine learning(ML) and artificial intelligence(AI) models offer more accurate forecasts than traditional statistical models \cite{tshipata2024multi}. Though these models come with a higher complexity and higher price for computation, however their increased accuracy offsets the negatives.

 In this literature  review we will look at the statistical, intelligent and hybrid models that have been used for the purpose of prediction. We will also look at the different methods that have been used for data pre-processing. This is because most models strive to get the highest accuracy value whilst, treating all data equally in the training process and not looking at the anomalies and different seasonal variations that represent themselves in  data \cite{wang2023improving}.
 
 
 
 \section{Statistical Models }
 Statistical methods have been the backbone of STLF before the prevalence of machine learning technologies. Different statistical models boast the ability to catch correlation in time series data. The methods have successfully been able to predict load with minimal error in the short and long term dimension. Though statistical methods are very old and have more cons that newer machine learning and hybrid models, they are still very relevant. Rusina et al, \cite{Rusina2022ShorttermLFH} highlights that despite their disadvantages and low performances for time series data, statistical models still have a, "fast implementation in practice, a wide range of use and are well studied". In this section we will review some statistical models that have been used and are still in use and look at how they hold up with current advancements.
 
 \subsubsection{Regression Models }

 Regression algorithms are mathematical tools that establish statistical correlations between variables, providing an insight on how each of the data parameters are related to the output data \cite{gochhait2023regression}. It also allows using a lot of predictors and it will predict outcome when there are a lot of independent predictors. A regression model being fundamentally based on the principal of finding correlation between input parameters and the output \cite{vardhan2023comparative}, it works well only until our dataset has flactuating data. Dhaval et al, \cite{Dhaval2020ShorttermLFB} used Multiple Linear Regression (MLR) in their study which was successfully trained and optimized to give a 95\% accuracy. Datasets for the STLF usually contains a lot of different data points and having an MLR allows us to consider each of the features to be added to the contribution.
 
 Gochhait et al ,\cite{gochhait2023regression} did a study to evaluate the most effective model out of 24 statistical models for their functionality. Amongst them were regression trees , linear trees exponential and quadratic trees just to name a few. These different regression models were tested under the same hardware conditions and the data pre-processing techniques and only 6 out of the 24 were shortlisted as the best performing , however the Rational Quadratic Gaussian Process Regression and the Exponential Gaussian Process Regression were the best rated regression models to use. These two models showed low error percentages. They benchmarked these models using a Support Vector Machine(SVM) model and the regression models outperformed the all the other models that were tested. Though the regression model produces what may look like good results they have limitations. Most regression models are ,"parametric linear models and can capture only linear dependencies between the current sample and historical data",\cite{tshipata2024multi}. This means that regression models struggle when the data has nonlinear dependencies. The solution to the ineffectiveness of linear regression models could be using SVM. This is because SVMs help in classification problems where LR models fail to provide clear boundaries. An SVM is capable of finding the best separating boundary by identifying an optimal hyperplane that distinguishes between data points or fits a regression function \cite{hussien2021comparative}. This will then classify the input data buy the separation brought by the hyperplane. In the case of data that is not perfectly separable, SVMs can find soft margins to classify data points. We can understand the nonlinear aspects of the forecasts using SVMs, however they become ineffective with very large datasets \cite{vardhan2023comparative}.
 
 
 
 \subsubsection{Exponential Smoothing }\label{sec:exponential smoothing}
 Exponential smoothing(ES) is a simple, low cost and adaptable statistical method that is used for time series forecasting. The concept behind ES is that the weights of the observed time series are exponentially decreased as observations come further in the past \cite{ramos2015performance}.
 The most recent time points get the highest weights while the older time get progressively smaller weights \cite{ramos2015performance}. There are different version of exponential smoothing. The first one being Simple Exponential Smoothing(SES), it puts focus on estimating the smoothed value of the series at a given time. The forecast that the SES will give for the next time step is determined by a smoothing constant \cite{ahmed2020review}. Ramos et al \cite{ramos2015performance} explains in his paper how ES methods are differentiated by their components and they simplified the components into three. The level components which would focus on the smoothed average of the series, which would be the basis of all models. The second component is trend, which is the rate of change in the data and finally the seasonality component which finds the recurring patterns in the data over a period. A combination of each of these components will end up giving us different ES models that specify on a particular component. The SES only uses the level component and this is a problem as it assumes that the data fluctuates around the same mean with no trend and seasonality, which in reality is not true because load data is volatile and irregular \cite{boopathy2024deep}.
 
Rendon et al. \cite{rendon2019structural}, using the component model, also applied the Holt-Winters model, which is well-suited for seasonal time series data and can be extended with an auto-regressive error correction term. Unlike Simple Exponential Smoothing, which only captures the level component, the Holt-Winters model includes parameters for both the level and seasonal components. This allows it to better handle series where seasonality plays a major role. Furthermore, they explored the Double-Seasonal Holt-Winters-Taylor model, which is specifically designed to capture two different seasonal patterns. This feature makes it particularly effective for short-term load forecasting (STLF), where electricity demand often exhibits both daily and weekly seasonal cycles.

 ES models have several advantages. They are very simple to set up and have a quick learning capability  , they have a straightforward structure \cite{tshipata2024multi}.These models are also capable of capturing seasonal and trend variations. They are well suited for time series data that exhibits a strong trend of seasonal patterns which are very common in electric load data \cite{ramos2015performance}.
 
Despite the strengths ES models face limitations when looking at the complex characteristics of electric load data.These models are essentially based on linear analysis and struggle to capture the inherent non-linearity and high volatility present in electricity load time series, which are influenced by numerous factors like weather, holidays, and user habits \cite{tshipata2024multi}.Their performance can deteriorate with highly irregular and random time series data \cite{wang2019novel} , essentially saying there are sensitive to noise and irregularities.Finally ES models generally tend to produce less accurate results that the "black box" methods used by utility companies\cite{takeda2016using}.
 
 \subsubsection{Auto Regressive Integrated Moving Average}
 The Autoregressive Integrated Moving Average (ARIMA) model is a widely recognised statistical approach employed for time series forecasting, particularly in STLF within power systems. It is considered a linear model that combines autoregressive(AR), difference(l) and moving average (MA) components \cite{revathi2025short}. It is valued as a simple and efficient model and is primarily excellent at capturing linear relations ad periodic patterns in time series data \cite{ramos2015performance}.
 
 The  ARIMA framework is often referred to as the Box-Jenkins model, it is systematic and practical and involves 3 iterative steps as discussed below.
 
\textbf{Step 1 : Model Identification} The initial stage involves analyzing time series data to determine the appropriate orders (p,d , q) for the AR , l and MA components.The AR component signifies that the current value of the time series (Yt)  is expressed linearly in terms of its 'p' previous values and a random noise component \cite{dai2020short}.The MA component denotes that the current value of Yt is a linear combination of white noise error terms (et) from previous time steps. The integrated (I) component (represented by 'd') signifies that the time series has been differenced 'd' times to achieve stationarity \cite{ahmed2020review}.

 \textbf{Step 2 : Parameter Estimation} Once the model structure (p, d, q) is identified, the parameters for the AR and MA components are estimated. This is often achieved by maximising the log-likelihood function \cite{ramos2015performance}.
 
 \textbf{Step 3 : Model Diagnosis} The final stage involves assessing the fit of the model by checking if the residuals are uncorrelated (i.e., behave like white noise). If the residuals are not uncorrelated, the model needs to be refined, requiring a return to the identification or estimation steps \cite{ramos2015performance}.
 
 ARIMA has been extended in multiple studies to enhance its capabilities . The Seasonal Auto-Regressive Integrated Moving Average (SARIMA) , is especially designed to handle seasonality in the data such as looking at daily patterns recognized in the data \cite{abbas2025self}.Eventually a SARIMA model can effectively eliminate the influence of periodicity in the prediction and it has shown great results in the past tests \cite{wang2012application}.
 
 In a study conducted by Wang et al  \cite{wang2012application} where they were looking at the performance of three residual modifications for improving SARIMA, they implemented an optimized fourier residual modification and this residual improved the outputs accuracy more than the normal SARIMA. The SARIMA/ARIMA are statistical models and carry the same disadvantages, even though it has higher accuracy that ES models it is still inferior to artificial neural network(ANN) and SVM models \cite{jiang2016short}.This is mainly because of its inability to capture non-linearity in the data \cite{he2019hybrid} and they are also limited in capturing long term dependencies \cite{abbas2025self}. In the study by Jiang et al \cite{jiang2016short} they found that ARIMA is less accurate, but notably more computationally efficient (11.25 seconds for ARIMA versus 683.62 seconds for ANN and 1412.7 seconds for GA-SVM). Hybrid ARIMA models would outperform standard ARIMA models however they would still not perform on the same level to machine learning models.
 

 
 

 \section{Intelligent Models}
 The emergence of intelligent methods in STLF has significantly advanced the field  , moving beyond traditional statistical models to embrace computational techniques that can better handle the complex and non-linear nature of electricity demand \cite{arvanitidis2021enhanced}.
 
 At the dawn of Artificial intelligence(AI) were foundational models and tools that were used and implemented for STLF to ensure accuracy.The early AI applications marked a crucial shift from purely statistical approaches that were limited by computing capabilities and often struggled with the non-linear features of time series data \cite{wang2018short}. These models aimed to address these limitations by leveraging their ability to learn patterns from complex data but they did not do so well due to non-linearity of the data.
 
 We will start by looking at very fundamental models that have been used for prediction and how they have impacted the industry  , we will expand further into looking at the more advanced models that are currently being implemented and tested in modern day technology.
 
 \subsection{Early Artificial Intelligence(AI) and Machine Learning Models}
 
 \paragraph{Support Vector Machines (SVMs)}

 SVMs  and their variants such as the Support Vector Regression(SVR) and Least Squares SVMs (LSSVM) were among the first prominent techniques used in STLF \cite{wang2018short}.The basic idea of an SVM for regression , is to ,"map the data x into a high-dimensional feature space via a nonlinear
 mapping and to perform a linear regression in this feature space"\cite{mohandes2002support}. This basically simplifies to an SVM being used in a classification task to choose a boundary that will maximize the margin that classifies an element in the dataset.
 
 SVMs are renowned for their kernel trick that effectively handles non-linear input spaces and provides proficient prediction models for regression problems like load forecasting\cite{hussien2021comparative}.They overcome overfitting issues through kernel methods and regularization techniques \cite{hussien2021comparative} , however a drawback is identified in their ineffectiveness or intolerable long training times when dealing with large datasets \cite{dong2017short}.Despite this SVMs have shown superiority compared to traditional statistical methods when it comes to load forecasting \cite{gochhait2023regression}.
 
 \paragraph{Fuzzy Logic methods} also emerged as early AI tools for STLF. Fuzzy logic is an extension of boolean logic however instead of having true or false(0 or 2)  , fuzzy logic allows variability of the truth ranging between 0 and 1. For example a pot can be 0.7 for "hot" and 0.3 for "warm".Fuzzy logic approches used fuzzy rules to integrate historical load data with time and day characteristics to determine probable load curves\cite{rafi2021short}.
 
 
 \paragraph{Artificial Neural Networks(ANNs)}represented a significant leap forward due to their ability to model complex non-linear  relationships between loads and related factors \cite{dong2017short}. Mimicking the human brain's information processing, ANNs can approximate non-linear functions with high fidelity and accuracy, making them well suited for  load forecasting \cite{arvanitidis2021enhanced}. ANNs gained their popularity due to their flexibility and robustness in handling diverse input-output projections and architectures. The earliest forms of ANNs for short term load forecasting showed up in the early 90's in the form of Multi Layer Perceptrons(MLP) \cite{arvanitidis2021enhanced}.Park et al \cite{arvanitidis2021enhanced} provided a 3 layer perceptron that used historical load data and temperature data to predict peak load, total daily load, and hourly load.Mandal et al also utilized a back-propagation neural network and this was noted to be the firstly widely used ANN method for STLF \cite{wu2020short}.Early ANN models have evolved to become more complex and capable for tackling task as load forecasting with ease.
 
 
 \subsubsection{Challenges and Drawbacks of Early AI Models}
 Despite the initial promise , standard ANNs presented key problems that limited their efficiency for time series tasks like STLF.
 These Methods had \textit{inefficiencies of Back-propagation}. The training of ANNs, especially those relying on backpropagation (BP) algorithms, suffered from issues such as slow convergence rates and high computational costs \cite{dong2017short}. ANNs were also prone to getting stuck in local optima and exhibiting overfitting, which could lead to poor generalization on new, unseen data \cite{wang2023short}. The final results of ANN models couldd also be dependent on the initial random weights and thresholds, contributing to forecast instability \cite{arvanitidis2021enhanced}.
 
 The early models also had a \textit{Lack of Temporal Memory}, which is a model's capacity to retain and utilise information from previous time steps when processing current and future data, which is crucial for capturing long-term dependencies and patterns that evolve over time\cite{zhu2025novel}. A fundamental limitation of standard ANNs for time series forecasting tasks was always their inability to learn from the sequential nature of the load data.Typical these models would treat each of the time steps individually when the time points are largely correlated.This would result in failure to memorize past information or account for long-term dependencies inherent in time varying electricity consumption patterns \cite{wang2023short}. This meant that they struggled to capture the influence of previous timesteps on current or future load values , hindering accurate predictions for dynamic systems.
 
 \subsection{Advanced Deep Learning Architecture for Enhanced STLF}
 
 Deep learning techniques, characterized by a significantly higher number of layers, offered a powerful solution to these limitations, enabling models to deal with complicated non-linear patterns and learn complete probability distributions with temporal memory from vast datasets \cite{tshipata2024multi}.The vast data that is generated daily by smart grids and Internet of Things (IOT) devices makes deep learning models ideal due to their scalability.Deep learning methods often offer better performance than more conventional machine learning methods \cite{ibrahim2022machine}.
 
 \subsubsection{Recurrent Neural Networks and Long Short Term Memory}
 
 Recurrent Neural Networks (RNNs) were introduced as a direct solution to the temporal memory problem in neural networks.Unlike the feed-forward networks, RNNs establish weight connections between layers over time, allowing them to reflect sequential information and possess a "memory ability" \cite{wang2018short}.This made these suitable for time series data, where the current outputs depend on the previous inputs and hidden states.This is because RNNs introduce a loop so that the network can remember information from previous steps.
 
 However, original RNNs still faced the vanishing gradient problem, where the ability of later time nodes to perceive previous ones decreased as the network deepened, limiting their performance with long time sequences \cite{wang2018short}.
 
 To address this, the Long Short-Term Memory (LSTM) network architecture was proposed, enhancing RNNs with recurrent gates (input, output, and forget gates) to solve the vanishing gradient problem and effectively deal with long-term dependencies \cite{wang2023short}.LSTM networks contain memory cells that store information over random time intervals, and gates that trace the flow of input and output data from the cell, allowing them to determine what information to discard, store, or output selectively \cite{he2019hybrid}.Studies have consistently demonstrated LSTM's effectiveness, showing lower errors and higher accuracy in STLF compared to traditional ANN approaches \cite{ahmed2020review}\cite{boopathy2024deep}. They are particularly capable of handling more complex time series load data with long-term dependencies \cite{rafi2021short}.LSTM has shown superior performance for longer forecast horizons compared to ARMA models \cite{tshipata2024multi} and this is because of their temporal capabilities.
 
 \subsubsection{Bidirectional LSTM (Bi-LSTM)}

 Bidirectional LSTM (Bi-LSTM) networks significantly improve upon the standard LSTM by processing data in both forward and backward directions \cite{wang2023short}. This architecture consists of two independent LSTM layers that run in opposite directions, both connected to the same output layer \cite{moradzadeh2021deep}. This allows the model to leverage both past context (from the forward pass) and future context (from the backward pass) for more accurate and robust predictions. The bidirectional movement and interconnected structure help eliminate problems such as missing data and overfitting in the training phase \cite{moradzadeh2021deep}. Research has shown Bi-LSTM to achieve superior performance, often with significantly lower Mean Absolute Percentage Error (MAPE) values, compared to unidirectional LSTMs and other methods \cite{ibrahim2022machine}.
 
 \subsubsection{Deep Belief Network (DBN)}\label{lit:dbn}

 Deep Belief Networks (DBNs) emerged as a crucial solution to address some of the challenges associated with backpropagation, particularly the difficulty of finding optimal initial parameters and the problem of local optima \cite{kong2019improved}. DBNs are generative probabilistic models composed of stacked Restricted Boltzmann Machines (RBMs) and a classifier. 
 DBNs combine deep learning with feature learning  , giving them powerful fitting capabilities to quickly analyze large amounts of data \cite{kong2019improved}.The multiple layers in DBNs help make them better in handling multifactor relationships in comparison  to single layer networks.Their greedy, layer-wise unsupervised pre-training mechanism helps in identifying better initial parameters, which are then fine-tuned through supervised learning \cite{kong2019improved}.
  This pre-training process allows DBNs to learn patterns progressively and overcome the disadvantage of being easily trapped in local optima due to random weight initialization. By converting continuous input features into binomial distribution features (e.g., using Gauss-Bernoulli RBMs), DBNs can improve their effectiveness in processing real-valued data \cite{kong2019improved}. This approach leads to faster convergence and improved prediction performance in various fields, including load forecasting.   This makes them ideal for uncovering electricity usage patterns and are considered ideal due to their scalability \cite{boopathy2024deep}.
 
 
 \subsubsection{Other Advanced Models}
  
  \paragraph{Gated Recurrent Unit (GRU)} neural networks were proposed as a simpler alternative to LSTM  , combining the input and forget gates into a single "update gate"  \cite{wang2018short}. GRUs can achieve similar accuracy to LSTMs with less computational cost and faster convergence \cite{hiceemdanQteg}. They are effective at extracting temporal features and preserve important features while using fewer parameters \cite{hiceemdanQteg}.
  
  \paragraph{Convolutional Neural Networks (CNNs)} have been frequently used in load prediction due to their excellent ability to capture the trend of load data. CNNs are particularly effective for feature extraction and dimension reduction of original data \cite{hiceemdanQteg}. CNN's have a lack of temporal memory that put them at a disadvantage in forecasting tasks  separately that require memory.They are often integrated with other models like LSTMs to leverage both spatial feature extraction and temporal modeling capabilities \cite{rafi2021short}. A method by Shafiul Hasan Rafi proposes an integrated CNN and LSTM network for STLF in the Bangladesh power system, reporting higher precision and accuracy than existing approaches like LSTM, Radial Basis Function Network (RBFN), and Extreme Gradient Boosting (XGBoost) \cite{rafi2021short}.
 
 
 \section{ Hybrid Models } 
 
 Hybrid models have emerged as a prominent approach in Short-Term Load Forecasting (STLF) to address the inherent complexities of electricity demand prediction, such as its non-linear and non-stationary characteristics \cite{dong2021short}.These models combine the strengths of various algorithms and techniques to enhance forecasting accuracy and reliability, overcoming the limitations often encountered by single, standalone models.
 
 The fundamental motivation is that no single forecasting technique is superior in all cases \cite{li2023short}. By combining methods, hybrid models can effectively handle the non-linear and multivariate characteristics of load data, extract complex patterns, and improve prediction accuracy and stability \cite{li2023short}. Conceptually, they operate by breaking down the forecasting problem, applying specialized techniques to different aspects (e.g., feature extraction, temporal learning, error correction), and then integrating the results. This often involves preprocessing data, applying deep learning for feature learning and sequence modeling, and optimizing parameters \cite{kong2019improved}.
 
 \subsection{Decomposition-Based Hybrid Models } 
 
A common approach involves decomposing the original, complex load data into simpler, more stable components using techniques like Variational Mode Decomposition (VMD) \cite{he2019hybrid},Empirical Mode Decomposition (EMD), or complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) \cite{hiceemdanQteg}.These methods will solve the problem of analyzing time series data directly and solve this by splitting the data into intrinsic components that are easier for machine learning models to learn from. Each component is then predicted using an appropriate model like an LSTM for high-frequency components, MLR for low-frequency components \cite{huang2023two} and the results are aggregated for the final forecast. This pre-processing step significantly reduces volatility and improves prediction accuracy \cite{wang2023short}.

\subsection{Deep Learning Combination Hybrid Models}
  
 \subsubsection{CNN-LSTM/GRU}
 
  Hybrid models like CNN-LSTM and CNN-GRU are frequently used. The CNN module excels at extracting local features and patterns from time series data, while the LSTM or GRU module is proficient in capturing long-term temporal dependencies\cite{zhu2025novel}.The combination of such models help get the benefits of both models in a prediction task.
  
  The CNN module is typically used to extract local features and patterns from load data \cite{wu2020short}. This is achieved through pooling and convolution layers which process two-dimensional data and flatten it into a one-dimensional feature vector.This feature vector is then fed as input to the LSTM or GRU layers, which are good at capturing long-term dependencies and sequential information in time series data \cite{zhu2025novel}. LSTM mechanisms allow for the retaining of long-term information and address the vanishing gradient problem \cite{zhu2025novel}. The GRU would combine with the CNN in the same way as an LSTM as they have a similar structure just simpler and fewer parameters, making it faster\cite{wang2018short}.
  
  This combination improved the forecast accuracy, reducing prediction errors and outperforming single models. In the tests done by  Rafi et al \cite{rafi2021short} , they found that CNN-LSTM models have a higher precision and accuracy in short-term load forecasting compared to standalone LSTM, Radial Basis Function Network (RBFN), and Extreme Gradient Boosting (XGBoost) approaches. A CNN-LSTM model provided 173.76 MW less Mean Absolute Error (MAE), 330.2 MW less Root Mean Squared Error (RMSE), and 3.07\% less Mean Absolute Percentage Error (MAPE) on average than a standalone LSTM network.In another study done by Danish et al \cite{danish2025kolmogorov} they found that the GRU-CNN model outperfomed a GRU , CNN and a Back Propagation Neural Network(BPNN) and this was due to its capability to learn from both time sequence data and spatiotemporal data.
  

 \subsubsection{DBN-RNN/Bi-RNN}
 Deep Belief Networks (DBNs) are integrated with Recurrent Neural Networks (RNNs) or Bidirectional RNNs (Bi-RNNs). This hybrid model typically leverages teg  DBNs capability for unsupervised pre-training and optimal parameter seeking with the RNN's strength in processing sequential and time-dependent data \cite{tang2019application}.
 
 A DBN being a generative and probabilistic model composed of multiple layers of RBM. These RBM combine the traditional neural networks with energy and probabilistic models  \cite{dong2021short}.The RBM is crucial for initialising parameters and this pretraining helps overcome issues like local minimisation and slow convergence seen in some other neural network \cite{gao2021cooling}. On the other side the RNN is specifically designed to process sequence data due to their internal loop structure that allows information to persist across time steps \cite{wang2018short} and this makes them highly effective for STLF where mining large quantities of temporal data is essential. A Bi-RNN is just a structural improvement that will process the input simultaneously in two opposite directions (forward and backward) \cite{tang2019application} . This bidirectional structure gives the RNN complete past and future context information for each point in the input layer  , enhancing the validity of the forecasting model \cite{tang2019application}.
 
 
 In the test done by Tang et al \cite{tang2019application} they used the DBN to perform the unsupervised greedy pre-training layer by layer to obtain initial weight parameters, making it easier to approach optimal values.The pretraining phase helped in handling parameters initialization problem in a large dataset. After this pre-training the parameters are supervisedly adjusted through the Bi-RNN, which then propagates learned time information in both directions to derive the final forecasted value. 
 In addition to model design, the study also incorporated data preprocessing techniques. Specifically, the bisecting k-means algorithm was applied for clustering, while Ensemble Empirical Mode Decomposition (EEMD) was used to decompose the load data into intrinsic mode functions (IMFs). This decomposition helped reduce noise and enhance feature selection, with methods such as the Pearson correlation coefficient further refining the input features. This data preprocessing played a part in the final perfomance of this hybrid model.
 The results of this study show that DBN with Bi-RNN reduces the MAPE to 1.85\% from 2\% which is a move towards the right direction.The  DBN-Bi-RNN models have been shown to outperform unidirectional LSTM, SVR, and BPNN \cite{tang2019application}.
 
 The DBN-RNN/Bi-RNN hybrid models provide a robust and accurate solution for STLF by combining DBN's parameter optimisation and feature learning capabilities with RNN/Bi-RNN's strength in handling temporal dependencies and contextual information. They often outperform traditional and even some advanced deep learning models in terms of forecasting accuracy and computational efficiency \cite{dong2021short}.
 
 
 
 \section{Data Pre-Processing Techniques}
 
 While machine learning and statistical models are powerful, their effectiveness depends heavily on the quality and structure of the input data. Proper data preprocessing is therefore essential to ensure that models operate at their full potential. In the context of electricity load forecasting, preprocessing addresses challenges arising from the complex, non-linear, non-stationary, and often noisy nature of load data factors influenced by weather conditions, consumer behavior, and other external variables \cite{revathi2025short}. A variety of preprocessing methods and techniques have been developed to tackle these challenges, and in this section we highlight some of them, examining their role and impact on model performance.
 
 \subsubsection{Data Cleaning and Outlier Detection}
Large datasets often contain incorrect, missing, or anomalous values, commonly referred to as data outliers. In machine learning models such as LSTMs, which depend heavily on the accuracy of historical data, the presence of outliers can significantly degrade model performance. To address this, techniques like the Hampel Identifier (HI) and the Interquartile Range (IQR) method are commonly applied to detect and correct or smooth anomalous data points \cite{hiceemdanQteg}.
 
The HI method was used in a study by Wang et al \cite{hiceemdanQteg}. This method applies a sliding window mechanism and replaces the extreme values with more representative estimates. For each data point , the median of the surrounding window is calculated along with the median absolute deviation(MAD), which will serve as a variability measure. They then scale the MAD to approximate the standard deviation.If a sample differs from the local median by more than three scaled deviations, it is identified as an outlier and replaced with the window median. Incorporating HI into data pre-processing ensures that outliers are corrected, thereby preventing disruptions in model training and improving the overall fitting performance.

To handle missing values in the dataset which may be very common due to defective sensors or erronous recordings, these can be addressed by filling in these missing values using zeros, means, median, linear interpolations or by just outright deleting the data points \cite{revathi2025short}.

Cleaning and detecting and removing outliers improve the quality and accuracy of the data preventing them from disrupting the model training, negatively impacting forecast accuracy \cite{hiceemdanQteg}. Pre-processing can reduce the data complexity, as evidenced by a lowered sample entropy (SampEn) value in \cite{hiceemdanQteg} indicating a higher degree of self-similarity and better data quality. Pre-processing with outlier correction improves the non-linear fitting performance of data and enhances prediction accuracy of a model.

\subsubsection{Data Normalization and Transformation}

The dataset that is collected has multiple data points that are assigned different values and weights. Each of the featurea should be treated withing the same scaling of importance. "The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values",\cite{kappal2019data}. This ensures that no single data point carries an advantage because of its numerical size.

Normalization will scale numerical input features to a uniform range, typically between 0 and 1, or to have zero mean and unit variance \cite{dong2021short}. Normalization can also be implemented using the min-max normalization method that will scale the data to a specified range. In \cite{dong2021short} this method is used to preprocess historical load, temperature and other features and it is very important in neural networks because of their sensitivity to variation in the input data. Feature scaling can also be done and this generally refers to scaling independent features to prevent biases from varying dimensions and ensure features are on a uniform scale \cite{zhu2025novel}.
 
\subsubsection{Feature Engineering and Selection}

Feature Engineering is the process of transforming raw data into meaningful features that better represent the underlying patterns \cite{boopathy2024deep}. Different prediction models perform best with different types of data. For example a CNN would perform best for image recognition meaning if we can change our raw data into an image. We can use a CNN for detection and prediction. Feature engineering can also be done by using some signal processing techniques to turn your data into signals that would then be used to train the models.


Variation Mode Decomposition (VMD) is one of the methods used for feature engineering. VMD is an adaptive signal decomposition method that iteratively searches the variation mode to decompose an original time series signal into a discrete number of modes, each with a limited bandwidth and a corresponding center frequency \cite{huang2023two}. The primary objective of VMD is to decompose the original signal into a set of modes where each mode has the smallest possible frequency range, while ensuring that all these modes, when put back together, exactly reproduce the original signal \cite{wang2023short}. VMD works by iteratively finding the optimal centre frequency and bandwidth for each mode and it does this by shifting the frequency of each mode to a "baseband" using a specific mathematical adjustment. After this it measures the spread of frequencies (bandwidth) for that shifted mode using a smooth, bell-shaped curve and this process is repeated many times \cite{wang2023short} until a predefined stop condition is met, ensuring adaptive decomposition of the signal \cite{wang2023short}.

when we use VMD on data we have a very strong anti noise capability and it reduces the complexity that is present in natural time series data  \cite{huang2023two}. VMD overcomes the modal mixing problem that is inherent in Empirical Mode Decomposition(EMD) and we can artificially change the number of mode decomposition which is an advantage over EMD \cite{huang2023two}. 


\subsection{Summary}

Studies around the area of STLF have shown a very clear path towards machine learning and AI algorithms being the future of prediction. The promise of these methods have been significantly highlighted for delivering more accurate load forecasts compared to traditional statistical methods. While the newer models may involve higher complexity and computational costs, their enhanced accuracy is seen as offsetting the negatives.There has also been a clear overarching between the statistical, hybrid and intelligent models along with data preprocessing techniques with the sole goal of higher accuracy and handling data anomalies and seasonal variations in the data.
 
The review has recognized LSTMs for their satisfactory performance in short term forecasting and capabilities to capture non-linear and temporal characteristics present in data. It has been shown that a combination of an LSTM with a CNN can also bring about a combination of benefits possessed by both models. This model can also be combined with purely statistical models to create hybrid models.  A variation of LSTM the Bi-LSTM was also identified as a potential for the future of LSTM in STLF in microgrids.Each of the variations of the STLF came with its own benefits in advancing load foresting. DBNs also showed a potential of being developed further to provide quality results that would be implemented into an accurate model of the future. DBNs also perform a lot better when they are presented in the Bi-Directional version of themselves.

This literature review has shown the capabilities of these advanced models effectively capturing non-linearities, long-range dependencies, and handle large-volumes of  time-series data, often benefiting from hybrid approaches and optimization techniques to deliver superior predictive performance.