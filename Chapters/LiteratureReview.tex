

\chapter{Literature Review}
\subsection{Introduction}
An odd power surge phenomenon was recognized during the 2008 Olympics in the UK. At certain times in  the evening there would be massive power surges at what seemed like random times in the evening. Though they seemed random to the utility companies they realized something rather funny was happening at these random times. Making a cup of tea is deeply ingrained in the english culture , after looking at the data they realized that the power surge was happening during times when a TV commercial would come on. Whenever a commercial came on TV multiple families would switch on their kettles to make a cup of tea. Though a single kettle may seem harmless  , but millions of kettles switched on at the same time would cause a large effect on the load of the power grid. This phenomenon was called the \textbf{\textit{'Great British Kettle Surge'}} \cite{kettle_surge}.\textit{\textbf{you can also talk about the blackouts mentioned in \cite{li2023ultra}}}

The effect of this great kettle surge can be very harmful to the grid if necessary steps are not taken to increase the grids capacity in times when we expect the power of the grid to increase suddenly. This is where the concept of load forecasting comes into play.
Electric Load forecasting is the process of predicting how much electricity will be needed at a given time and how that demand will affect the utility grid \cite{IBM_loadforecasting}. To bring this matter close to home , South Africa has been struggling with provision of power to its population. This could be accredited to the lack of proper load forecasting in the previous years. A Growing population results in a growth in the demand of electricity. If the country does not build facilities to supply enough electricity for the load requirements then  load-shedding becomes the only solution to protect the grid. In a perfect country the advice from load forecasters in the early 2000's would have been taken into consideration and built more facilities to meet this demand.
The above example shows the hand in hand relationship between load forecasts and their economic impacts.Large forecasting errors may lead to either excessively risky or excessively conservative
scheduling, which can in turn result in undesirable economic penalties\cite{festas2001computational}.An addition in accuracy of 15 can save the UK 10 000 000 euros\cite{gochhait2023regression}. This means that there is a massive push towards finding the best load forecasting techniques that can be accurate and reliable.

Load forecasting is separated into 3 main categories which are Short , Medium and Long term Load Forecasting. the major differentiator between the three is the duration in which the forecasting is predicted for.

\textbf{Long Term Load Forecasting(LTLF)} considers periods that are more than a year. LTLF mainly considers factors such as demographic changes, economic growth and energy policy impacts \cite{IBM_loadforecasting}. This forecasting helps utilities think of what can be done to improve their systems to meet the increasing demand of the grid in the future.
\textbf{Medium Term Load Forecasting (MTLF)} forecasts look at periods between a few months and a year. MTLF is important for demand side management  , storage maintenance and scheduling of power \cite{han2018enhanced}.

Finally we have short term load forecasting(STLF) which looks at shorter time periods from hourly, daily all the way up to a week of load prediction.STLF is essential in daily operation performance , such as load flow and estimating how many power generators can be used in a particular day \cite{shohan2022forecasting}. If there is efficient model for STLF  , problems such as \textit{british kettle surge} can easily be planned for ahead by ensuring more generators are operational when the demand for power rises.STLF can also ensure that the grid has a reliable continuous power flow during power shortages or outages\cite{tarmanini2023short}.

In this study we would like to look at how we can improve STLF using machine learning and deep learning model approaches.There has been a clear identification in most of the current studies that Artificial Neural Networks offer a more accurate forecast than traditional statistical models. The major tradeoff between these two methods would be accuracy and computational power.

 In this literature  review we will look at the Statistical , Intelligent and Hybrid models that have been used for the purpose of prediction.We will also look at the different methods that have been used for data processing. This is because most models strive to get the highest accuracy value whilst , treating all data equally in the training process and not looking at the anomalies and different seasonal volatilities that represent themselves in this data \cite{wang2023improving}
 
 
 
 \section{Statistical Models }
 Statistical methods have been the backbone of STLF before the prevalence of machine learning technologies.Different statistical models boast the ability to catch correlation in time series data.The methods have successfully been able to predict load with minimal error in the short and long term dimension. Though statistical methods are very old and have more cons that newer machine learning and hybrid models  , they are still very relevant. \cite{Rusina2022ShorttermLFH} higlights that despite their disadvantages and low performances for time series data  , statistical models still have a, "fast implementation in practice , a wide range of use and are well studied".In this section we will review some statistical models that have been used and are still in use and look at thow they hold up with current advancements
 
 \subsubsection{Regression Models }

 Regression algorithms are mathematical tools that establish statistical correlations between variables  , providing an insight on how each of the data parameters are related to the output data \cite{gochhait2023regression}.It also allows using a lot of predictors and it will predict outcome when there are a lot of independent predictors. A regression model being fundamentally based on the principal of finding correlation between input parameters and the output \cite{vardhan2023comparative} , it works well only until our dataset has flactuating data. \cite{Dhaval2020ShorttermLFB} used Multiple Linear regression in their study which was successfully trained and optimized to give a 95\% accuracy. Datasets for the STLF usually contains a lot of different data points and having an MLR allows us to consider each of the features to be added to the contribution.
 
 \cite{gochhait2023regression} did a study to evaluate the most effective out of 24 statistical models for their functionality. amongst them were regression trees , linear trees exponential and quadratic trees just to name a few. These different regression models were tested under the same hardware conditions and the data pre-processing techniques and only 6 out of the 24 were shortlisted as the best performing , however the Rational Quadratic GPR and the Exponential GPR were the best rated regression models to use. These two models showed low error percentages. They Benchmarked these models using an SVM and the regression models outperformed them.Though the regression model produces what may look like good results they have massive limitations. Most regression models are ,"parametric linear models and can capture only linear dependencies between the current sample and historical data",\cite{tshipata2024multi}.This means that regression models struggle when the data has nonlinear dependencies.The solution to the ineffectiveness of linear regression models could be using Support Vector Machines(SVM).This is because SVM's help in classification problems where LR models fail to provide clear boundaries.A SVM is capable of finding the best seperating boundary between different classes in the input data.\textbf{find source}. In the case of data that is not perfectly seperable like the data we use for STLF SVM's can find soft margins to classify data points.Even though we would be able to solve for thee nonlinear aspects of the forecasts using SVM's they become ineffective with very large datasets \cite{vardhan2023comparative}.
 
 
 
 \subsubsection{Exponential Smoothing }
 Exponential smoothing(ES) is a simple  ,low cost and adaptable statistical method that is used for time series forecasting.The concept behind ES is that the  weights of the observed time series are exponentially decreased as observations come further in the past \cite{ramos2015performance}.
 The most recent time points get the highest weights while the older time get progressively smaller weights\cite{ramos2015performance}.There are different version of exponential smoothing. The first one being Simple Exponential Smoothing , this is in its most basic form , and it puts focus on estimating the smoothed value of the series at a given time. The forecast that the SES will give for the next time step is determined by a smoothing constant\cite{ahmed2020review}.\cite{ramos2015performance} explains in his paper how Es methods are differentiated by their components and he simplified the components into three. The Level components which would focus on th esmoothed average of the series  , which would be the basis of all models  The second component is trend , which is the rate of change in the data and finally the Seasonality component which finds the recurring patterns in the data over a period. A combination of each of these components will end up giving us different ES models that specify on a particular component. The SES only uses the level component and this is a problem.
 
 \cite{rendon2019structural} using the component model also used the Holt-Winters model that is used for the seasonal time series data  , and can be augmented with an auto-regressive error correction term.This model contains parameters for the level and seasonal components. This change separates them from the Simple ES because it can predict on both of these components rather than one.Finally they also tried the Double-Seasonal Holt-Winters-Taylor model, this model captures double seasonality found in higher frequency electricity demand series data. This makes it a good method for STLF.

 ES models have several advantages. They are very simple to set up and have a quick learning capability  , they have a straightforward structure \cite{tshipata2024multi}.These models are also capable of capturing seasonal and trend variations. They are well suited for time series data that exhibits a strong trend of seasonal patterns which are very common in electric load data \cite{ramos2015performance}.
 
Despite the strengths ES models face limitations when looking at the complex characteristics of electric load data.These models are essentially based on linear analysis and struggle to capture the inherent non-linearity and high volatility present in electricity load time series, which are influenced by numerous factors like weather, holidays, and user habits \cite{tshipata2024multi}.Their performance can deteriorate with highly irregular and random time series data \cite{wang2019novel} , essentially saying there are sensitive to noise and irregularities.Finally ES models generally tend to produce less accurate results that the "black box" methods used by utility companies\cite{takeda2016using}.
 
 \subsubsection{Auto Regressive Integrated Moving Average}
 The Autoregressive Integrated Moving Average (ARIMA) model is a widely recognised statistical approach employed for time series forecasting, particularly in Short-Term Load Forecasting (STLF) within power systems. It is considered as a linear model that combines autoregressive(AR)  , difference(l) and moving average (MA) components \cite{revathi2025short}.Its valued as a simple and efficient model   and is primarily excellent at capturing linear relations ad periodic patterns in time series data \cite{ramos2015performance}.
 
 The  ARIMA framework is often refered as the Box-Jenkins model , its systematic and practical and involves 3 iterative steps.
 
\textbf{Step 1 : Model Identification} The initial stage invloves analysing time series data to determine the appropriate orders (p,d , q) for the AR , l and MA components.The AR component signifies that the current value of the time series (Yt)  is expressed linearly in terms of its 'p' previous values and a random noise component \cite{dai2020short}.The MA component denotes that the current value of Yt is a linear combination of white noise error terms (et) from previous time steps. The ntegrated (I) component (represented by 'd') signifies that the time series has been differenced 'd' times to achieve stationarity \cite{ahmed2020review}.

 \textbf{Step 2 : Parameter Estimation} Once the model structure (p, d, q) is identified, the parameters for the AR and MA components are estimated. This is often achieved by maximising the log-likelihood function \cite{ramos2015performance}.
 
 \textbf{Step 3 : Model Diagnosis} The final stage involves assessing the fit of the model by checking if the residuals are uncorrelated (i.e., behave like white noise). If the residuals are not uncorrelated, the model needs to be refined, requiring a return to the identification or estimation steps \cite{ramos2015performance}.
 
 ARIMA has been extended in multiple studies to enhance its capabilities . The Seasonal Auto-Regressive Integrated Moving Average (SARIMA) , is especially designed to handle seasonality in the data such as looking at daily patterns recognized in the data \cite{abbas2025self}.Eventually a SARIMA model can effectively eliminate the influence of periodicity in the prediction and it has shown great results in the past tests \cite{wang2012application}.
 
 In a study conducted by Y. wang et al  \cite{wang2012application} where they were looking at the perfomance of three residual modification for improving SARIMA.In this study they implemented an optimized Fourier residual modification and this residual improved the outputs accuracy more than the normal SARIMA.The SARIMA/ARIMA are statistical models and carry the same disadvantages.Even though it has higher accuracy that ES models it is still inferior to ANN and SVM models \cite{jiang2016short}.In the study by Jiang et al \cite{jiang2016short} they found that ARIMA less accurate, but notably more computationally efficient (11.25 seconds for ARIMA versus 683.62 seconds for ANN and 1412.7 seconds for GA-SVM). Hybrid ARIMA models would outperform standar ARIMA models however they would still perform inferior to machine learning models.
 
 
 \subsubsection{Conclusion for statistical methods} 
 The statistical models offer a low cost of computation and easy implementation of LF.However since they fail to effectively capture nonlinear characteristics of load data they fail to satisfy the accuracy and stability that is required by the grid management in the modern day \cite{li2023short}.
 \newpage
 

 \section{Intelligent Models}
 The emergence of intelligent methods in STLF has significantly advanced the field  , moving beyond traditional statistical models to embrace computational techniques that can better handle the complex and non-linear nature of electricity demand \cite{arvanitidis2021enhanced}.
 
 At the dawn of Artificial intelligence were foundational models and tools that were used and implemented to held STLF accuracy.The early AI applications marked a crucial shift from purely statistical approaches that were limited by computing capabilities and often struggled with the non-linear features of time series data \cite{wang2018short}. These models aimed to address these limitations by leveraging their ability to learn patterns from complex data but they did not do so well due to no linearity.
 
 We will start by looking at very fundamental models that have been ued for prediction and how they have impacted the industry  , we will expand further into looking at the more advanced models that are currently being implemented and tested in modern day technology.
 
 \subsection{Early Artificial Intelligence(AI) Models}
 
 \paragraph{Support Vector Machines (SVM)}

 SVM's  and their variants such as the Support Vector Regression(SVR) and Least Squares Support Vector Machine (LSSVM) were among the first prominent techniques used in STLF \cite{wang2018short}.The basic idea of an SVM for regression , is to ,"map the data x into a high-dimensional feature space via a nonlinear
 mapping and to perform a linear regression in this feature space"\cite{mohandes2002support}. This basically simplifies to an SVM being used in a classification task to choose a boundary that will maximize the margin that classifies an element in the dataset.
 
 SVM's are renowned for their kernel trick that effectively handles non-linear input spaces and provides proficient prediction models for regression problems like load forecasting\cite{hussien2021comparative}.They overcome overfitting issues through kernel methods and regularization techniques \cite{hussien2021comparative} , however a drawback is identified in their ineffectiveness or intolerable long training times when dealing with large datasets \cite{dong2017short}.Despite this SVM , has shown superiority to traditional statistical methods when it comes to load forecasting \cite{gochhait2023regression}.
 
 \paragraph{Fuzzy Logic methods} also emerged as early AI tools for STLF. Fuzzy logic is an extension of boolean logic however instead of having true or false(0 or 2)  , fuzzy logic allows variability of the truth ranging between 0 and 1. For example a pot can be 0.7 for "hot" and 0.3 for "warm".Fuzzy logic approches used fuzzy rules to integrate historical load data with time and day characteristics to determine probable load curves\cite{rafi2021short}.
 
 
 \paragraph{Artificial Neural Networks} represented a significant leap forward due to their ability to model complex non-linear  relationships between loads and related factors \cite{dong2017short}. Mimicking the human brain's information processing  , ANN's can approximate non-linear functions with high fidelity and accuracy , making them well suited for  load forecasting \cite{arvanitidis2021enhanced}.Ann's gained their popularity due to their flexibility and robustness in handling diverse input-output projections and architectures. The earliest forms of ANN's for short term load forecasting showed up in the early 90's in the form of Multi Layer Perceptrons(MLP) \cite{arvanitidis2021enhanced} but have since grown into more advanced models.
 
 
 \subsubsection{Challenges and Drawbacks of Early AI Models}
 Despite the initial promise , standard ANNs presented key problems that limited their efficiency for time series tasks like STLF.
 These Methods had \textit{inefficiencies of Back-propagation}. The training of ANNs, especially those relying on backpropagation (BP) algorithms, suffered from issues such as slow convergence rates and high computational costs \cite{dong2017short}. ANNs were also prone to getting stuck in local optima and exhibiting overfitting, which could lead to poor generalization on new, unseen data \cite{wang2023short}. The final results of ANN models couldd also be dependent on the initial random weights ad thresholds , contributing to forecast instability \cite{arvanitidis2021enhanced}.
 
 The early models also had a \textit{Lack of Temporal Memory}. A fundamental limitation of standard ANNs for time series forecasting tasks was always their inability to learn from the sequential nature of the load data.Typical these models would treat each of the time steps individually when the time points are largely correlated.This would result in failure to memorize past information or account for long-term dependencies inherent in time varying electricity consumption patterns \cite{wang2023short}. This meant that they struggled to capture the influence of previous timesteps on current or future load values , hindering accurate predictions for dynamic systems.
 
 \subsection{Advanced Deep Learning Architecture for Enhanced STLF}
 
 Deep learning techniques, characterized by a significantly higher number of layers, offered a powerful solution to these limitations, enabling models to deal with complicated non-linear patterns and learn complete probability distributions with temporal memory from vast datasets \cite{tshipata2024multi}.The vast data that is generated daily by smart grids and IoT devices makes deep learning models ideal due to their scalability.Deep Learning methods often offer better performance than more conventional machine learning methods \cite{ibrahim2022machine}.
 
 \subsubsection{Recurrent Neural Networks and Long Short Term Memory}
 
 Recurrent Neural Networks (RNNs) were introduced as a direct solution to the temporal memory problem in neural networks.Unlike the feed-forward networks  , RNNs establish weight connections between layers over time, allowing them to reflect sequential information and possess a "memory ability" \cite{wang2018short}.This made these suitable for time series data , where the current outputs depend on the previous inputs and hidden states.This is because RNNs introduce a loop so that the network can remember information from previous steps.
 
 However, original RNNs still faced the vanishing gradient problem, where the ability of later time nodes to perceive previous ones decreased as the network deepened, limiting their performance with long time sequences \cite{wang2018short}.
 
 To address this, the Long Short-Term Memory (LSTM) network architecture was proposed, enhancing RNNs with recurrent gates (input, output, and forget gates) to solve the vanishing gradient problem and effectively deal with long-term dependencies \cite{wang2023}.LSTM networks contain memory cells that store information over random time intervals, and gates that trace the flow of input and output data from the cell, allowing them to determine what information to discard, store, or output selectively \cite{he2019hybrid}.Studies have consistently demonstrated LSTM's effectiveness, showing lower errors and higher accuracy in STLF compared to traditional ANN approaches. They are particularly capable of handling more complex time series load data with long-term dependencies \cite{rafi2021short}.LSTM has shown superior performance for longer forecast horizons compared to ARMA models \cite{tshipata2024multi} and this sis because of their temporal capabilities.
 
 \subsubsection{ Bidirectional LSTM (Bi-LSTM)}
 \textbf{add an image for LSTM and BI-LSTM to show their difference} \\
 Bidirectional LSTM (Bi-LSTM) networks significantly improve upon the standard LSTM by processing data in both forward and backward directions \cite{wang2023short}. This architecture consists of two independent LSTM layers that run in opposite directions, both connected to the same output layer \cite{moradzadeh2021deep}. This allows the model to leverage both past context (from the forward pass) and future context (from the backward pass) for more accurate and robust predictions. The bidirectional movement and interconnected structure help eliminate problems such as missing data and overfitting in the training phase \cite{moradzadeh2021deep}. Research has shown Bi-LSTM to achieve superior performance, often with significantly lower Mean Absolute Percentage Error (MAPE) values, compared to unidirectional LSTMs and other methods \cite{ibrahim2022machine}.
 
 \subsubsection{Deep Belief Network (DBN)}

 Deep Belief Networks (DBNs) emerged as a crucial solution to address some of the challenges associated with backpropagation, particularly the difficulty of finding optimal initial parameters and the problem of local optima \cite{kong2019improved}. DBNs are generative probabilistic models composed of stacked Restricted Boltzmann Machines (RBMs) and a classifier. 
 DBNs combine deep learning with feature learning  , giving them powerful fitting capabilities to quickly analyze large amounts of data \cite{kong2019improved}.The multiple layers in DBNs help make them better in handling multifactor relationships in comparison  to single layer networks.Their greedy, layer-wise unsupervised pre-training mechanism helps in identifying better initial parameters, which are then fine-tuned through supervised learning \cite{kong2019improved}.
  This pre-training process allows DBNs to learn patterns progressively and overcome the disadvantage of being easily trapped in local optima due to random weight initialization. By converting continuous input features into binomial distribution features (e.g., using Gauss-Bernoulli RBMs), DBNs can improve their effectiveness in processing real-valued data \cite{kong2019improved}. This approach leads to faster convergence and improved prediction performance in various fields, including load forecasting.   This makes them ideal for uncovering electricity usage patterns and are considered ideal due to their scalability \cite{boopathy2024deep}.
 
 
 \subsubsection{Other Advanced Models}
  
  \paragraph{Gated Recurrent Unit (GRU)} neural networks were proposed as a simpler alternative to LSTM  , combining the input and forget gates into a single "update gate"  \cite{wang2018short}. GRUs can achieve similar accuracy to LSTMs with less computational cost and faster convergence \cite{hiceemdanQteg}. They are effective at extracting temporal features and preserve important features while using fewer parameters \cite{hiceemdanQteg}.
  
  \paragraph{Convolutional Neural Networks (CNN)} have been frequently used in load prediction due to their excellent ability to capture the trend of load data. CNNs are particularly effective for feature extraction and dimension reduction of original data \cite{hiceemdanQteg}. CNN's have a lack of temporal memory that put them at a disadvantage in forecasting tasks  separately that require memory.They are often integrated with other models like LSTMs to leverage both spatial feature extraction and temporal modeling capabilities \cite{rafi2021short}. A method by Shafiul Hasan Rafi proposes an integrated CNN and LSTM network for STLF in the Bangladesh power system, reporting higher precision and accuracy than existing approaches like LSTM, Radial Basis Function Network (RBFN), and XGBoost \cite{rafi2021short}.
 
 
 \section{ Hybrid Models } 
 
 Hybrid models have emerged as a prominent approach in Short-Term Load Forecasting (STLF) to address the inherent complexities of electricity demand prediction, such as its non-linear and non-stationary characteristics \cite{dong2021short}.These models combine the strengths of various algorithms and techniques to enhance forecasting accuracy and reliability, overcoming the limitations often encountered by single, standalone models.
 
 The fundamental motivation is that no single forecasting technique is superior in all cases \cite{li2023short}. By combining methods, hybrid models can effectively handle the non-linear and multivariate characteristics of load data, extract complex patterns, and improve prediction accuracy and stability \cite{li2023short}. Conceptually, they operate by breaking down the forecasting problem, applying specialized techniques to different aspects (e.g., feature extraction, temporal learning, error correction), and then integrating the results. This often involves preprocessing data, applying deep learning for feature learning and sequence modeling, and optimizing parameters \cite{kong2019improved}.
 
 \subsection{Decomposition-Based Hybrid Models } 
 
A common approach involves decomposing the original, complex load data into simpler, more stable components using techniques like Variational Mode Decomposition (VMD) \cite{he2019hybrid},Empirical Mode Decomposition (EMD), or complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) \cite{hiceemdanQteg}.These methods will solve the problem of analyzing time series data directly and solve this by splitting the data into intrinsic components that are easier for machine learning models to learn from. Each component is then predicted using an appropriate model like an LSTM for high-frequency components, Multiple Linear Regression (MLR) for low-frequency components \cite{huang2023two} and the results are aggregated for the final forecast. This pre-processing step significantly reduces volatility and improves prediction accuracy \cite{wang2023short}.

\subsection{Deep Learning Combination Hybrid Models}
  
 \subsubsection{CNN-LSTM/GRU}
 
  Hybrid models like CNN-LSTM and CNN-GRU are frequently used. The CNN module excels at extracting local features and patterns from time series data, while the LSTM or GRU module is proficient in capturing long-term temporal dependencies\cite{zhu2025novel}.The combination of such models help get the benefits of both in a prediction task.
  
  The CNN module is typically used to extract local features and patterns from load data \cite{wu2020short} . This is achieved through pooling and convolution layers which process two-dimensional data and flatten it into a one-dimensional feature vector.This feature vector is then fed as input to the LSTM or GRU layers  , which are good at capturing long-term dependencies and sequential information in time series data \cite{zhu2025novel}.LSTM mechanism allow for the retaining of long-term informationand address the vanishing gradient problem \cite{zhu2025novel}.The GRU would combine with the CNN in the same way as an LSTM as they have a similar structure just simpler and fewer parameters , making it faster\cite{wang2018short}.
  
  Such a combination will improve the forecast accuracy  , reduce prediction errors and outperform single models.In the tests done by  Rafi et al \cite{rafi2021short} , they found that CNN-LSTM models have a higher precision and accuracy in short-term load forecasting compared to standalone LSTM, Radial Basis Function Network (RBFN), and Extreme Gradient Boosting (XGBoost) approaches. A CNN-LSTM model provided 173.76 MW less Mean Absolute Error (MAE), 330.2 MW less Root Mean Squared Error (RMSE), and 3.07\% less Mean Absolute Percentage Error (MAPE) on average than a standalone LSTM network.In another study done by Danish et al \cite{danish2025kolmogorov} they found that the GRU-CNN model outperfomed a GRU , CNN and a Back Propagation Neural Network(BPNN) and this was due to its capability to learn from both time sequence data and spatiotemporal data.
  

 \subsubsection{DBN-RNN/Bi-RNN}
 Deep Belief Networks (DBNs) are integrated with Recurrent Neural Networks (RNNs) or Bidirectional RNNs (Bi-RNNs). This hybrid model typically leverages teg  DBNs capability for unsupervised pre-training and optimal parameter seeking with the RNN's strength in processing sequential and time-dependent data \cite{tang2019application}.
 
 A DBN being a generative and probabilistic model composed of multiple layers of RBM. These RBM combine the traditional neural networks with energy and probabilistic models  \cite{dong2021short}.The RBM is crucial for initialising parameters and this pretraining helps overcome issues like local minimisation and slow convergence seen in some other neural network \cite{gao2021cooling}. On the other side the RNN is specifically designed to process sequence data due to their internal loop structure that allows information to persist across time steps \cite{wang2018short} and this makes them highly effective for STLF where mining large quantities of temporal data is essential. A Bi-RNN is just a structural improvement that will process the input simultaneously in two opposite directions (forward and backward) \cite{tang2019application} . This bidirectional structure gives the RNNa complete past and future context information for each point in the input layer  , enhancing the validity of the forecasting model \cite{tang2019application}.
 
 
 In the test done by tang et al \cite{tang2019application} they used the DBN to perfom the unsupervised greedy pre-training layer by layer to obtain initial weight parameters, making it easier to approach optimal values.The pretraining phase helped in handling parameters initialization problem in a large dataset.After this pre-training the parameters are supervisedly adjusted through the Bi-RNN, which then propagates learned time information in both directions to derive the final forecasted value. In the same study they also incorporated data preprocessing which are Bisecting K-Means Algorithm for clustering and Ensemble Empirical Mode Decomposition (EEMD) for decomposing load data into intrinsic mode functions (IMFs), is employed to reduce noise and enhance feature selection using methods like Pearson correlation coefficient. This data preprocessing played a part in the final perfomance of this hybrid model.
 The results of this study show that DBN with Bi-RNN reduces the MAPE to 1.85\% from 2\% which is a move towards the right direction.The  DBN-Bi-RNN models have been shown to outperform unidirectional LSTM, SVR, and BPNN \cite{tang2019application}.
 
 The DBN-RNN/Bi-RNN hybrid models provide a robust and accurate solution for STLF by combining DBN's parameter optimisation and feature learning capabilities with RNN/Bi-RNN's strength in handling temporal dependencies and contextual information. They often outperform traditional and even some advanced deep learning models in terms of forecasting accuracy and computational efficiency \cite{dong2021short}.
 
 
 
 \section{Data Pre-Processing Techniques}
 
 

 \newpage
 \begin{itemize}
 \item \cite{wang2023short} talks about a data preocessing and CNN-BI-LSTM
 \item	paper on \textbf{(HI-CEEMDAN-Q-TEG) } \cite{hiceemdanQteg} important
 \item  this paper looks at an alternate\textbf{ data preprocessing} method, it shows an improvement after adding sampling  \cite{wang2023improving}.
 \item \textbf{Data Processing} focuses on using sliding fuzzy granulation for data processing \cite{li2023short}.Offers the benefits for using statistical models.Li also mentions how hybrid models have been able to improve the forecast capabilities of the models .
 \item \textbf{data preprocessing} \cite{he2019hybrid} it uses a VMD method.it is also connected to the \cite{hiceemdanQteg}
 \item \textbf{Data preprocessing} \cite{wang2019novel} using the LMD processing technique 
 \item\textbf{ Data preprocosessing} \cite{ahmed2020review} 3.2. Weather Classification
 Weather status "is an indispensable pre-processing step, especially for short-term PVPF (ST-PVPF)." PV output is "most strongly correlated with solar spectral irradiance," which depends on meteorological factors (aerosol distribution, wind speed and direction, humidity, and cloud cover).
 
 Benefits: Integrating weather classification improves model robustness and forecast precision.
 Challenges: Insufficient training datasets for all weather types, particularly extreme conditions. Generative Adversarial Networks (GANs) are recommended for augmenting these datasets.
 Methodology: Most studies categorise weather into fewer than four types, though some reclassify up to 10. "Finer the weather classification; better the prediction results."
 Cloud Motion Study: Crucial for intra-hour forecasts. Ground-based sky images offer higher resolution than satellite imagery for regional cloud formations. Techniques like "image phase shift invariance (IPSI) based CMDV calculation married to Fourier phase correlation theory (FPCT)" are used.
 \item \cite{tshipata2024multi} showed how much superior an LSTM is to the traditional ARMA model
 \item \cite{li2023ultra}  \textbf{CEEDMAN}
 
 \end{itemize}
 


